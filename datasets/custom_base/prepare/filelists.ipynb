{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare filelists for LJSpeech dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md\n",
    "dir_data = \"../../../dataset_uz2\"\n",
    "audio_dir = \"dataset_uz2/audio\"\n",
    "config = \"../config.yaml\"\n",
    "# symlink = \"DUMMY1\"\n",
    "n_val = 100\n",
    "n_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hyperparameters from config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.hparams import get_hparams_from_file\n",
    "\n",
    "hps = get_hparams_from_file(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Here `normalized_text` contains numbers in the form of words.\n",
    "\n",
    "**Note**: you may need to replace all `\"|\"` with `\" | \"` in the file `metadata.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_uz2/audio/utt_0000.wav</td>\n",
       "      <td>U do‘kondan non sotib oldi.</td>\n",
       "      <td>u do‘kondan non sotib oldi.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_uz2/audio/utt_0001.wav</td>\n",
       "      <td>Bugun havo juda iliq.</td>\n",
       "      <td>bugun havo juda iliq.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_uz2/audio/utt_0002.wav</td>\n",
       "      <td>kVt.s elektr energiya yetkazib bergan.</td>\n",
       "      <td>kilovatt-soniya elektr energiya yetkazib bergan.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_uz2/audio/utt_0003.wav</td>\n",
       "      <td>Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...</td>\n",
       "      <td>joriy yilda bu raqam ikki milliard kilovatt.so...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_uz2/audio/utt_0004.wav</td>\n",
       "      <td>Yangi elektr uzatish liniyasini barpo etish, O...</td>\n",
       "      <td>yangi elektr uzatish liniyasini barpo etish, o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  \\\n",
       "0  dataset_uz2/audio/utt_0000.wav   \n",
       "1  dataset_uz2/audio/utt_0001.wav   \n",
       "2  dataset_uz2/audio/utt_0002.wav   \n",
       "3  dataset_uz2/audio/utt_0003.wav   \n",
       "4  dataset_uz2/audio/utt_0004.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0                        U do‘kondan non sotib oldi.   \n",
       "1                              Bugun havo juda iliq.   \n",
       "2             kVt.s elektr energiya yetkazib bergan.   \n",
       "3  Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...   \n",
       "4  Yangi elektr uzatish liniyasini barpo etish, O...   \n",
       "\n",
       "                                     normalized_text  cleaned_text  \n",
       "0                        u do‘kondan non sotib oldi.           NaN  \n",
       "1                              bugun havo juda iliq.           NaN  \n",
       "2   kilovatt-soniya elektr energiya yetkazib bergan.           NaN  \n",
       "3  joriy yilda bu raqam ikki milliard kilovatt.so...           NaN  \n",
       "4  yangi elektr uzatish liniyasini barpo etish, o...           NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    f\"{dir_data}/uzbek_tts_output.csv\",\n",
    "    sep=r\"|\",\n",
    "    header=None,\n",
    "    names=[\"file\", \"text\", \"normalized_text\", \"cleaned_text\"],\n",
    "    index_col=False,\n",
    "    # converter to add .wav to file name\n",
    "    # converters={\"file\": lambda x: f\"{symlink}/{x.strip()}.wav\", \"text\": str.strip, \"normalized_text\": str.strip},\n",
    "    converters={\"file\": lambda x: f\"{audio_dir}/{x.strip()}.wav\", \"text\": str.strip, \"normalized_text\": str.strip},\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaners\n",
    "\n",
    "It may take a while, so better to preprocess the text and save it to a file in advance.\n",
    "\n",
    "**Note** `phonemize_text` takes the longest time.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenize_text', 'add_bos_eos']\n",
      "[['phonemize_text'], ['add_spaces']]\n"
     ]
    }
   ],
   "source": [
    "# Get index of tokenize_text\n",
    "text_cleaners = hps.data.text_cleaners\n",
    "\n",
    "token_idx = text_cleaners.index(\"tokenize_text\")\n",
    "token_cleaners = text_cleaners[token_idx:]\n",
    "print(token_cleaners)\n",
    "\n",
    "\n",
    "# Extract phonemize_text\n",
    "def separate_text_cleaners(text_cleaners):\n",
    "    final_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for cleaner in text_cleaners:\n",
    "        if cleaner == \"phonemize_text\":\n",
    "            if temp_list:\n",
    "                final_list.append(temp_list)\n",
    "            final_list.append([cleaner])\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(cleaner)\n",
    "\n",
    "    if temp_list:\n",
    "        final_list.append(temp_list)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "text_cleaners = text_cleaners[:token_idx]\n",
    "text_cleaners = separate_text_cleaners(text_cleaners)\n",
    "print(text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/econrich/.conda/envs/vits23/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/econrich/.conda/envs/vits23/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning with ['phonemize_text'] ...\n",
      "Cleaning with ['add_spaces'] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_uz2/audio/utt_0000.wav</td>\n",
       "      <td>U do‘kondan non sotib oldi.</td>\n",
       "      <td>u do‘kondan non sotib oldi.</td>\n",
       "      <td>ʊ &lt;space&gt; d ɔ ʔ k ˈɔ n d a n &lt;space&gt; n ˈɔ n &lt;s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_uz2/audio/utt_0001.wav</td>\n",
       "      <td>Bugun havo juda iliq.</td>\n",
       "      <td>bugun havo juda iliq.</td>\n",
       "      <td>b ˈu ɡ ʊ n &lt;space&gt; h ˈa v ɔ &lt;space&gt; j ˈu d a &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_uz2/audio/utt_0002.wav</td>\n",
       "      <td>kVt.s elektr energiya yetkazib bergan.</td>\n",
       "      <td>kilovatt-soniya elektr energiya yetkazib bergan.</td>\n",
       "      <td>k i l ˈo v a t t s o n ˈi j a &lt;space&gt; ˈe l ɛ k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_uz2/audio/utt_0003.wav</td>\n",
       "      <td>Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...</td>\n",
       "      <td>joriy yilda bu raqam ikki milliard kilovatt.so...</td>\n",
       "      <td>j ˈo ɹ ɪ j &lt;space&gt; j ˈɪ l d a &lt;space&gt; b ʊ &lt;spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_uz2/audio/utt_0004.wav</td>\n",
       "      <td>Yangi elektr uzatish liniyasini barpo etish, O...</td>\n",
       "      <td>yangi elektr uzatish liniyasini barpo etish, o...</td>\n",
       "      <td>j ˈa ŋ ɪ &lt;space&gt; ˈe l ɛ k t r &lt;space&gt; u z ˈa t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  \\\n",
       "0  dataset_uz2/audio/utt_0000.wav   \n",
       "1  dataset_uz2/audio/utt_0001.wav   \n",
       "2  dataset_uz2/audio/utt_0002.wav   \n",
       "3  dataset_uz2/audio/utt_0003.wav   \n",
       "4  dataset_uz2/audio/utt_0004.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0                        U do‘kondan non sotib oldi.   \n",
       "1                              Bugun havo juda iliq.   \n",
       "2             kVt.s elektr energiya yetkazib bergan.   \n",
       "3  Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...   \n",
       "4  Yangi elektr uzatish liniyasini barpo etish, O...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0                        u do‘kondan non sotib oldi.   \n",
       "1                              bugun havo juda iliq.   \n",
       "2   kilovatt-soniya elektr energiya yetkazib bergan.   \n",
       "3  joriy yilda bu raqam ikki milliard kilovatt.so...   \n",
       "4  yangi elektr uzatish liniyasini barpo etish, o...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ʊ <space> d ɔ ʔ k ˈɔ n d a n <space> n ˈɔ n <s...  \n",
       "1  b ˈu ɡ ʊ n <space> h ˈa v ɔ <space> j ˈu d a <...  \n",
       "2  k i l ˈo v a t t s o n ˈi j a <space> ˈe l ɛ k...  \n",
       "3  j ˈo ɹ ɪ j <space> j ˈɪ l d a <space> b ʊ <spa...  \n",
       "4  j ˈa ŋ ɪ <space> ˈe l ɛ k t r <space> u z ˈa t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "text_norm = data[\"normalized_text\"].tolist()\n",
    "for cleaners in text_cleaners:\n",
    "    print(f\"Cleaning with {cleaners} ...\")\n",
    "    if cleaners[0] == \"phonemize_text\":\n",
    "        text_norm = tokenizer(text_norm, Vocab, cleaners, language=hps.data.language)\n",
    "    else:\n",
    "        for idx, text in enumerate(text_norm):\n",
    "            temp = tokenizer(text, Vocab, cleaners, language=hps.data.language)\n",
    "            text_norm[idx] = temp\n",
    "\n",
    "data = data.assign(cleaned_text=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 90\n",
      "['<pad>', '<unk>', '<bos>', '<eos>', '<space>', '<laugh>', 'a', 'ɪ', 'l', 'n', 's', 't', 'ˈa', 'd', 'm', 'j', 'k', 'b', 'h', 'r', 'ɹ', 'ˈi', 'ɡ', 'q', 'ˈɪ', 'ɔ', 'ˈɔ', 'z', 'ʔ', 'i', 'ˌa', 'v', '.', 'tʃ', 'o', 'ʊ', 'ŋ', 'ˈo', 'ˈʊ', ',', 'ˌɔ', 'ˌi', 'χ', 'p', 'f', 'ˌu', 'u', 'ˌo', 'e', 'ˈe', 'ˈu', 'ˈɛ', 'ˌɪ', 'ɛ', 'ˌe', 'ˈæ', 'æ', 'ˌʊ', 'ˌɛ', '“', '”', 'ˌæ', 'ts', ':', '(en)', '(uz)', '?', '!', 'ə', 'ˈʌ', 'əl', 'ˌuː', 'ʃ', ';', 'ɐ', '\"', 'ˈy', 'x', 'ɯ', 'ˈaɪ', 'ˈɔː', '—', '…', 'oː', 'ɔː', 'c', 'ɟ', 'ˈeɪ', 'ˌɒ', 'ˌɔː']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab_uz.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token cleaners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_uz2/audio/utt_0000.wav</td>\n",
       "      <td>U do‘kondan non sotib oldi.</td>\n",
       "      <td>u do‘kondan non sotib oldi.</td>\n",
       "      <td>ʊ &lt;space&gt; d ɔ ʔ k ˈɔ n d a n &lt;space&gt; n ˈɔ n &lt;s...</td>\n",
       "      <td>2\\t35\\t4\\t13\\t25\\t28\\t16\\t26\\t9\\t13\\t6\\t9\\t4\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_uz2/audio/utt_0001.wav</td>\n",
       "      <td>Bugun havo juda iliq.</td>\n",
       "      <td>bugun havo juda iliq.</td>\n",
       "      <td>b ˈu ɡ ʊ n &lt;space&gt; h ˈa v ɔ &lt;space&gt; j ˈu d a &lt;...</td>\n",
       "      <td>2\\t17\\t50\\t22\\t35\\t9\\t4\\t18\\t12\\t31\\t25\\t4\\t15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_uz2/audio/utt_0002.wav</td>\n",
       "      <td>kVt.s elektr energiya yetkazib bergan.</td>\n",
       "      <td>kilovatt-soniya elektr energiya yetkazib bergan.</td>\n",
       "      <td>k i l ˈo v a t t s o n ˈi j a &lt;space&gt; ˈe l ɛ k...</td>\n",
       "      <td>2\\t16\\t29\\t8\\t37\\t31\\t6\\t11\\t11\\t10\\t34\\t9\\t21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_uz2/audio/utt_0003.wav</td>\n",
       "      <td>Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...</td>\n",
       "      <td>joriy yilda bu raqam ikki milliard kilovatt.so...</td>\n",
       "      <td>j ˈo ɹ ɪ j &lt;space&gt; j ˈɪ l d a &lt;space&gt; b ʊ &lt;spa...</td>\n",
       "      <td>2\\t15\\t37\\t20\\t7\\t15\\t4\\t15\\t24\\t8\\t13\\t6\\t4\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_uz2/audio/utt_0004.wav</td>\n",
       "      <td>Yangi elektr uzatish liniyasini barpo etish, O...</td>\n",
       "      <td>yangi elektr uzatish liniyasini barpo etish, o...</td>\n",
       "      <td>j ˈa ŋ ɪ &lt;space&gt; ˈe l ɛ k t r &lt;space&gt; u z ˈa t...</td>\n",
       "      <td>2\\t15\\t12\\t36\\t7\\t4\\t49\\t8\\t53\\t16\\t11\\t19\\t4\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  \\\n",
       "0  dataset_uz2/audio/utt_0000.wav   \n",
       "1  dataset_uz2/audio/utt_0001.wav   \n",
       "2  dataset_uz2/audio/utt_0002.wav   \n",
       "3  dataset_uz2/audio/utt_0003.wav   \n",
       "4  dataset_uz2/audio/utt_0004.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0                        U do‘kondan non sotib oldi.   \n",
       "1                              Bugun havo juda iliq.   \n",
       "2             kVt.s elektr energiya yetkazib bergan.   \n",
       "3  Joriy yilda bu raqam 2 mlrd kVt.soatni tashkil...   \n",
       "4  Yangi elektr uzatish liniyasini barpo etish, O...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0                        u do‘kondan non sotib oldi.   \n",
       "1                              bugun havo juda iliq.   \n",
       "2   kilovatt-soniya elektr energiya yetkazib bergan.   \n",
       "3  joriy yilda bu raqam ikki milliard kilovatt.so...   \n",
       "4  yangi elektr uzatish liniyasini barpo etish, o...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  ʊ <space> d ɔ ʔ k ˈɔ n d a n <space> n ˈɔ n <s...   \n",
       "1  b ˈu ɡ ʊ n <space> h ˈa v ɔ <space> j ˈu d a <...   \n",
       "2  k i l ˈo v a t t s o n ˈi j a <space> ˈe l ɛ k...   \n",
       "3  j ˈo ɹ ɪ j <space> j ˈɪ l d a <space> b ʊ <spa...   \n",
       "4  j ˈa ŋ ɪ <space> ˈe l ɛ k t r <space> u z ˈa t...   \n",
       "\n",
       "                                              tokens  \n",
       "0  2\\t35\\t4\\t13\\t25\\t28\\t16\\t26\\t9\\t13\\t6\\t9\\t4\\t...  \n",
       "1  2\\t17\\t50\\t22\\t35\\t9\\t4\\t18\\t12\\t31\\t25\\t4\\t15...  \n",
       "2  2\\t16\\t29\\t8\\t37\\t31\\t6\\t11\\t11\\t10\\t34\\t9\\t21...  \n",
       "3  2\\t15\\t37\\t20\\t7\\t15\\t4\\t15\\t24\\t8\\t13\\t6\\t4\\t...  \n",
       "4  2\\t15\\t12\\t36\\t7\\t4\\t49\\t8\\t53\\t16\\t11\\t19\\t4\\...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train, val, test filelists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"file\", \"tokens\"]]\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"../filelists/train.txt\", sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"../filelists/val.txt\", sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"../filelists/test.txt\", sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vits23)",
   "language": "python",
   "name": "vits23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
